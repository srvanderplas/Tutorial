{
  "hash": "0714dfb83b18455a17b6caab3879ce16",
  "result": {
    "markdown": "---\ntitle: \"Statistics Primer\"\neditor: visual\n---\n\n\n# Experimental Design and Statistical Analysis\n\n## What is Statistics?\n\n**Statistics** is the science concerned with developing and studying methods for collecting, analyzing, interpreting, and presenting empirical data.\n\nStatistics answers questions like\n\n-   How should I conduct experiments?\n-   What is the best way to test my hypothesis?\n-   What is the true value of \\_\\_\\_ and how precisely do I know that value?\n-   What does my experiment/sample value tell me about my data?\n-   What the value of \\_\\_\\_ will be in the (near) future\n\nStatistics concerns itself with several different (but related) tasks:\n\n+---------------------------------------------------------------+-------------------------------------------------------------------+\n| **Description**: What does the data say?                      | **Inference**: What does the data tell us (about the population)? |\n|                                                               |                                                                   |\n| ![](../image/histogram-sample-stats.png)                         | ![](../image/sample-population.png)                                  |\n+---------------------------------------------------------------+-------------------------------------------------------------------+\n| **Experimental Design**: What's the best way to collect data? | **Prediction**: What will happen next?                            |\n|                                                               |                                                                   |\n| ![](../image/wizard-list.png)                                    | ![](../image/crystal-ball-fuzzball.png)                              |\n+---------------------------------------------------------------+-------------------------------------------------------------------+\n\n## An Historical Example: The Logic of Statistical Hypothesis Testing\n\nSir Ronald Fisher once had a conversation with a woman who claimed to be able to tell whether the tea or milk was added first to a cup. Fisher, being interested in probability, decided to test this woman's claim empirically by presenting her with 8 randomly ordered cups of tea - 4 with milk added first, and 4 with tea added first. The women was then supposed to select 4 cups prepared with one method, but is allowed to directly compare each cup (e.g. tasting each cup sequentially, or in pairs).\n\n![](../image/tea-tasting-fuzzballs.png)\n\nThe lady identified each cup correctly. Do we believe that this could happen by random chance alone?[^1]\n\n[^1]: A more thorough explanation of this experiment is available [here](https://en.wikipedia.org/wiki/Lady_tasting_tea) - the tea tasting experiment is one of the pillars of early statistics and randomization-based hypothesis testing.\n\n### Initial statistical setup\n\n-   Null Hypothesis (what we are hoping to disprove):\\\n    There is no observable difference between the two methods of preparing tea (any success is by chance alone).\n\n-   Alternative Hypothesis (if random chance isn't the reason, then what?):\\\n    There is an observable difference between the two methods of preparing tea, that is, the woman can actually detect via taste which cups were prepared using either method.\n\nIf the results we observed are due to random chance alone, how common would it be to get 8/8 correct?\n\n### Simulation\n\n1.  Flip a coin 4 times to represent the 4 cups identified as containing tea with milk added first. Heads corresponds to a correct decision, Tails to an incorrect decision.\n\n2.  Record the number of successful selections\n\n3.  Repeat 100 times to generate a distribution of results under the null hypothesis -- that is, what our results would look like if the lady is just guessing.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Simulated results (left), and simulated results evaluated in light of our observed data.](2022-Extension-REU_files/figure-html/unnamed-chunk-1-1.png){width=49.5%}\n:::\n\n::: {.cell-output-display}\n![Simulated results (left), and simulated results evaluated in light of our observed data.](2022-Extension-REU_files/figure-html/unnamed-chunk-1-2.png){width=49.5%}\n:::\n:::\n\n\nYou can play with different configurations of the tea tasting scenario [here](https://srvanderplas.shinyapps.io/UNL-APL-workshop/) or [here](https://shiny.srvanderplas.com/APL/) (two links to different servers hosting the same application).\n\n### Strength of the Evidence\n\nOur simulation shows that under random chance, it appears that our tea taster would be expected to identify all of the cups correctly (corresponding to HHHH) 4/100 = 0.04 times.\n\n::: callout-note\n0.04 in this example corresponds to our **p-value** - the probability under the null hypothesis of observing the results from our experiment.\n:::\n\n0.04 is a fairly low value, so we might say that we don't believe that the results are sufficiently likely under our null hypothesis. In this case, the probability of observing the results under the null hypothesis is low, so we might instead conclude that the alternative hypothesis seems more reasonable -- that is, it is more likely that the null hypothesis is wrong. So, we might instead conclude that because our p-value is so low, that is, that we are unlikely to observe results this extreme under random chance, that instead we believe that the lady is not guessing and can actually detect the difference in tea prepared in different ways. We **reject the null hypothesis and conclude that the alternative is more likely**.\n\nNow, in this example, we have a relatively low sample size -- which means that it is hard for us to find evidence too much stronger than what our simulation provided for us. In many situations in science, however, we may require **stronger evidence** that the null hypothesis is unlikely to generate results that we have observed -- corresponding to requiring a p-value that is lower than 0.05. The correct threshold value (\"alpha\") for rejecting a null hypothesis differs by discipline (physics may use values like $10^{-6}$) and by the consequences of the results (medical trials may require small p-values, while studies looking for avenues to explore may use higher p-values.)\n\n### Connection to Theory-based Tests\n\nThe simulation method described above for evaluating the probability of observing the evidence under the null hypothesis relies on significant manual simulation or outsourcing that work to a computer. The results are not guaranteed to be the same every time, and before computers were common, the manual simulation approach was often too tedious to use regularly. Thus, probability distributions describing the results mathematically were used instead.\n\n![Simulated distributions approximate the theoretical distribution, but connect more clearly to real-world ideas.](../image/SimulationTheoryCartoon.png)\n\nNo matter what distribution the researcher uses to compare to, the basic idea of theory-based statistics is the same: we look for **evidence that our result is (or is not) likely under the null hypothesis**. If the result is likely under the null, then we can't say anything -- we can't disprove the idea that the results are due to chance. If the result isn't likely under the null, then we reject the null hypothesis and say the alternative is more likely.\n\nIn theory-based tests, this evidence is usually in the form of **a \\_\\_-value**, where the blank is the name of the distribution (commonly, $z$, $t$, $F$, $\\chi^2$).\n\n![The same process used in simulated distributions works in theory-based distributions.](../image/Stat_test_logic.png)\n\nYou can play with different simulations and distributions [here](https://srvanderplas.shinyapps.io/UNL-APL-workshop/) or [here](https://shiny.srvanderplas.com/APL/) (two links to different servers hosting the same application). Use the Distributions tab.\n\n### Understanding p-values\n\nThe p-value is the probability of observing the data we saw under the null hypothesis $H_0$. A p-value is the **area under the reference distribution** where values are **as or more extreme** than the hypothesized value.\n\n![](../image/p-value-logic.png)\n\nIf the p-value is low ($p < \\alpha$, where $\\alpha = 0.05, 0.01$, or another pre-specified value) then we know that it is relatively unlikely to observe our data under $H_0$... which means that it is more likely that $H_0$ is false and $H_A$ is true.\n\nWhen p-values are low, we **reject** $H_0$ and conclude that $H_A$ is more likely.\n\n![](../image/two-sided-test.png)\n\nNot all alternative hypotheses are one sided, e.g. $x > a$. Some hypotheses are two sided - $H_0: x = a$ and $H_A: x \\neq a$. When we have a two-sided hypothesis, we have two areas of our distribution that contribute to the p-value.\n\n## Estimating Values: Confidence Intervals\n\nSometimes, we don't want to test whether a parameter is equal to a specific value - instead, we might want to know what that value is (or at least, a range of possible values for that parameter).\n\nIn this case, we construct a **confidence interval** - a set of plausible values for the parameter.\n\nWe can think of a confidence interval as our best estimate of the parameter value + uncertainty.\n\n![Confidence intervals are a range of values around the central estimate obtained from the sample data](../image/confidence-interval-best-est-uncertainty.png)\n\nConfidence intervals, like hypothesis tests, are conducted based on a parameter $\\alpha$ representing the acceptable level of error. If we want to be 95% confident in our estimate, our interval will be **wider** than if we want to be 90% confident in our estimate -- we have to include more values to get a wider interval.\n\n## Experimental Design\n\nMany studies have a design which is more complicated than the simple examples shown in the previous section. However, all of these designs use the same statistical principles - they may just be applied in different ways.\n\nWhat follows is a brief description and depiction of how these different experimental designs work in practice. Focus primarily on the way the experiments are set up -- in the end, there is always a test statistic computed from the sample mean(s) and variance(s) which is compared to a distribution, as described above.\n\n### One Sample Tests\n\n#### Categorical Variables\n\nIn one-sample tests of categorical variables, we typically want to know whether the proportion of successes (the quantity we're interested in) is equal to a specific value (that is, $\\pi = 0.5$ or something of that sort). Our population parameter, $\\pi$, represents the unknown population quantity, and our sample statistic, $\\hat p$, represents what we know about the value of $\\pi$.\n\nIn these tests, our **null hypothesis** is that $\\pi = a$, where a is chosen relative to the problem. Often, $a$ is equal to 0.5, because usually that corresponds to random chance.\n\nWhen simulating these experiments, we will often use a coin flip (for random chance) or a spinner (for other values of $\\pi$) to generate data.\n\nYou can play with different one sample categorical tests [here](https://srvanderplas.shinyapps.io/UNL-APL-workshop/) or [here](https://shiny.srvanderplas.com/APL/) (two links to different servers hosting the same application). Use the One Categorical Variable tab.\n\n#### Continuous Variables\n\nOne-sample continuous variable experiments cannot be simulated because we do not usually know the characteristics of the population we're trying to predict from. Instead, we use theory-based tests for continuous one-sample data.\n\nYou can play with different one sample continuous tests [here](https://srvanderplas.shinyapps.io/UNL-APL-workshop/) or [here](https://shiny.srvanderplas.com/APL/) (two links to different servers hosting the same application). Use the One Continuous Variable tab.\n\n<!-- Two sample test example -->\n\n### Two Sample Tests\n\nIn a two-sample test, there are two groups of participants which are assigned different treatments. The goal is to see how the two treatments differ. Because there are two groups, the mathematical formula for calculating the standardized statistic is slightly more complicated (because the variability of $\\overline{X}_A - \\overline{X}_B$ is a bit more complicated), but in the end that statistic is compared to a similar reference distribution.\n\n![A two-sample test randomly divides up a sample into two groups and calculates the sample mean for each group.](../image/unpaired-test.png) <!-- Paired vs unpaired tests -->\n\nYou can play with different two sample categorical tests [here](https://srvanderplas.shinyapps.io/UNL-APL-workshop/) or [here](https://shiny.srvanderplas.com/APL/) (two links to different servers hosting the same application). Use the Categorical + Continuous Variable tab.\n\n### Matched Pairs and Repeated Measures\n\nIn a variation on this theme, in some cases, we may want to compare two treatments, but we may design the experiment to reduce variability using either repeated measures or matched pairs.\n\nA repeated measures study requires each participant to experience both treatments in the experiment.\n\n![In a repeated measures study, the same individuals see both treatment A and treatment B; this way, each individual serves as their own control](../image/repeated-measures.png)\n\nSometimes, it is not ethical or feasible to give the same individual both treatments -- for instance, we cannot ethically or logistically assign mothers to breastfeed or formula feed with the same baby -- and so instead, we must create pairs of individuals who are similar.\n\n![In a matched pairs study, we use covariates to match individuals and then assign each individual in a pair to a single treatment. Matched pairs studies reduce variability, but not as much as repeated measures studies](../image/matched-pairs.png)\n\nYou can play with different matched-pairs and repeated-measures studies [here](https://srvanderplas.shinyapps.io/UNL-APL-workshop/) or [here](https://shiny.srvanderplas.com/APL/) (two links to different servers hosting the same application). Use the One Continuous Variable tab, and the difference between your matched pairs or repeated measures as your observation.\n\n### Analysis of Variance\n\n<!-- Multiple variables (ANOVA) -->\n\nIn other cases, we may be more interested in sources of variability in the data.\n\n![Suppose we have a group of elementary school children, separated by grade. We want to know if grade is a good predictor of the children's height.](../image/anova-class.png)\n\nIf a variable is important, then it should account for more variability in the data than the overall variability between two observations. In this example, that means that children within a grade should be more similar than children who are from different grades.\n\n![To get this information, for each group we calculate the group mean](../image/anova-class-means.png)\n\n![Then, for each individual, we calculate the deviation from the group mean](../image/anova-class-deviations.png)\n\nANOVA models are sometimes shown in tables like this:\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n|Factor    | Df| Sum Sq|   Mean Sq| F value|   Pr(>F)|\n|:---------|--:|------:|---------:|-------:|--------:|\n|grade     |  2|    112| 56.000000|   26.25| 1.26e-05|\n|Residuals | 15|     32|  2.133333|        |         |\n|Total     | 17|    144|          |        |         |\n:::\n:::\n\n\nThe overall statistic we calculate is the ratio of the sum-of-squares within groups to the sum-of-squares between groups. The mathematical details of this aren't that important here, unless they make the process easier to understand.\n\nBetween group SS: $6 * (43.77 - 41)^2 + 6* (43 - 43.77)^2 + 6 * (47 - 43.77)^2 = 42.77 + 2.69 + 66.53 = 112$\n\nWithin group SS: $$\\left(0^2 + 3^2 + (-1)^2 + 0^2 + (-1)^2 + (-1)^2\\right) + \\\\\n\\left((-1)^2 + (-1)^2 + 2^2 + (-2)^2 + 1^2 + 1^2\\right) + \\\\\n\\left(0^2 + 0^2 + 0^2 + 0^2 + (-2)^2 + 2^2\\right) = 32$$\n\nSo by adding the group variable, we reduce our total error from 144 (112 + 32) to 32. Not bad! Put another way, our group variable explains some 77% of the overall variability in the data.\n\nANOVA models calculate out an F-statistic which is compared to an F-distribution. This F-statistic is calculated as the ratio of the Mean Sq. for each group - the sum of squared errors for each group (between, within) divided by the degrees of freedom (the number of observations - 1). In this case, our F value (with 2, 15 degrees of freedom) is 26.25. This corresponds to a p-value of 0.00001262 -- so the chances of observing something this significant when there is in fact no difference between groups is *very* low.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![It is clear that 26.25 is very much in the tails of the F(2, 15) distribution, which explains the extremely low p-value.](2022-Extension-REU_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=50%}\n:::\n:::\n\n\n### Linear Regression\n\n<!-- Regression Intro -->\n\n![Two continuous variables may have a linear relationship](../image/linear-trend.png)\n\nIt's fairly common in science to have two continuous quantitative variables -- for instance, height and weight, or fertilizer applied and yield. When we have data like this, we use linear regression to fit a regression line to the data. This line minimizes the errors in $y$, and is sometimes called the **least squares regression line**.\n\n![We estimate the linear relationship using the least-squares regression line $\\hat{y} = a \\cdot x + b$, where $a$ is the slope of the line and $b$ is the y-intercept. $\\hat y$ is the predicted value of y at a given value of $x$](../image/linear-regression-1.png)\n\nGenerally, though, our data are not fixed: they are the result of sampling. If this is the case, then we know that a different sample may provide a different result. Thus, we know that any estimates $a$ and $b$ have variability that directly results from **how we collected the data**.\n\n![If we have a different sample, we may get a different regression line -- which means $a$ and $b$ have variability (and a corresponding distribution)](../image/linear-regression-diff-lines.png)\n\nAs a result, we generally have a **distribution** of values corresponding to $a$ and a similar **distribution** of values corresponding to $b$. Because our data are the result of a random sample, we can analyze the least squares regression coefficients using statistical techniques.\n\nWe may want to test hypotheses about $a$ such as \"Is there a relationship between $x$ and $y$?\" (This doesn't mention $a$, but if there is no relationship between $x$ and $y$, then $a$ would be 0, so implicitly, we're asking whether $a = 0$.) Typically, scientists aren't as interested in the value of $b$.\n\nWe can test these hypotheses using simulation by arranging our data as if there is no relationship between $x$ and $y$ - that is, by shuffling our $y$ data to correspond to a random $x$ value in the dataset. Then, we would fit a new regression line and record the value of $a^\\ast$ (the simulated $a$).\n\nJust like before, when we shuffled data or labels, we generate a distribution of $a^\\ast$ under the null hypothesis and compare our observed value to that distribution, counting up the number of values which are more extreme.\n",
    "supporting": [
      "2022-Extension-REU_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}